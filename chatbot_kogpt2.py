"""
KoGPT2 ê¸°ë°˜ ë ˆì‹œí”¼ ì±—ë´‡ (ì™„ì „í•œ ë‹µë³€ ë²„ì „)
"""
import json
import torch
import os
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from config_kogpt2 import *

class KoGPT2RecipeChatbot:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ìƒíƒœ ì¶”ì 
        self.model_loaded = False
        self.is_trained_model = False
        
        self.load_model()
        self.load_recipe_data()
    
    def check_trained_model_exists(self):
        """í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        required_files = [
            'config.json',
            'pytorch_model.bin',  # ë˜ëŠ” model.safetensors
            'tokenizer.json',
            'tokenizer_config.json'
        ]
        
        if not os.path.exists(MODEL_PATH):
            print(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
            return False
        
        missing_files = []
        for file in required_files:
            file_path = os.path.join(MODEL_PATH, file)
            if not os.path.exists(file_path):
                # pytorch_model.bin ëŒ€ì‹  model.safetensorsê°€ ìˆì„ ìˆ˜ ìˆìŒ
                if file == 'pytorch_model.bin':
                    alt_path = os.path.join(MODEL_PATH, 'model.safetensors')
                    if os.path.exists(alt_path):
                        continue
                missing_files.append(file)
        
        if missing_files:
            print(f"âŒ ëˆ„ë½ëœ ëª¨ë¸ íŒŒì¼ë“¤: {missing_files}")
            return False
        
        print("âœ… í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•©ë‹ˆë‹¤")
        return True
    
    def load_model(self):
        """í›ˆë ¨ëœ KoGPT2 ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
        
        # 1ë‹¨ê³„: í›ˆë ¨ëœ ëª¨ë¸ ì¡´ì¬ í™•ì¸
        if self.check_trained_model_exists():
            print("ğŸ“‚ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            
            try:
                # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
                self.model = GPT2LMHeadModel.from_pretrained(
                    MODEL_PATH,
                    local_files_only=True  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
                )
                self.tokenizer = PreTrainedTokenizerFast.from_pretrained(
                    MODEL_PATH,
                    local_files_only=True
                )
                
                # GPUë¡œ ì´ë™
                self.model.to(self.device)
                self.model.eval()
                
                # íŒ¨ë”© í† í° ì„¤ì •
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                self.model_loaded = True
                self.is_trained_model = True
                
                print("âœ… í›ˆë ¨ëœ KoGPT2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                print(f"  - ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")
                print(f"  - ì–´íœ˜ í¬ê¸°: {len(self.tokenizer)}")
                print(f"  - íŒ¨ë”© í† í°: {self.tokenizer.pad_token}")
                
                # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
                self.test_model()
                return
                
            except Exception as e:
                print(f"âŒ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ”„ ê¸°ë³¸ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
        
        # 2ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        print("ğŸ“‚ ê¸°ë³¸ KoGPT2 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        try:
            self.model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)
            
            self.model.to(self.device)
            self.model.eval()
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model_loaded = True
            self.is_trained_model = False
            
            print("âœ… ê¸°ë³¸ KoGPT2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print("âš ï¸ ì£¼ì˜: í›ˆë ¨ë˜ì§€ ì•Šì€ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            print("   ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” model_trainer_kogpt2.pyë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”")
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {e}")
            self.model_loaded = False
            raise e
    
    def test_model(self):
        """ëª¨ë¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        try:
            test_prompt = "ì§ˆë¬¸: ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?\në‹µë³€:"
            
            input_ids = self.tokenizer.encode(test_prompt, return_tensors='pt').to(self.device)
            
            with torch.no_grad():
                output = self.model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + 30,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    do_sample=True
                )
            
            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def load_recipe_data(self):
        """ë ˆì‹œí”¼ ë°ì´í„° ë¡œë“œ (í´ë°±ìš©)"""
        try:
            with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:
                self.recipes = json.load(f)
            print(f"ğŸ“š ë ˆì‹œí”¼ ë°ì´í„° ë¡œë“œ: {len(self.recipes)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ë ˆì‹œí”¼ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.recipes = []
    
    def generate_answer(self, question):
        """KoGPT2ë¡œ ë‹µë³€ ìƒì„± - ê¸¸ì´ ì œí•œ ì—†ìŒ"""
        if not self.model_loaded:
            return None
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = PROMPT_FORMAT.format(question=question)
            
            # í† í¬ë‚˜ì´ì§•
            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
            
            # ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€
            if input_ids.shape[1] > MAX_LENGTH - GENERATION_MAX_LENGTH:
                input_ids = input_ids[:, -(MAX_LENGTH - GENERATION_MAX_LENGTH):]
            
            # ìƒì„± - ë” ê¸´ ë‹µë³€ í—ˆìš©
            with torch.no_grad():
                output = self.model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + GENERATION_MAX_LENGTH * 2,  # ë” ê¸´ ìƒì„±
                    temperature=GENERATION_CONFIG['temperature'],
                    top_p=GENERATION_CONFIG['top_p'],
                    top_k=GENERATION_CONFIG['top_k'],
                    repetition_penalty=GENERATION_CONFIG['repetition_penalty'],
                    do_sample=GENERATION_CONFIG['do_sample'],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True,
                    num_return_sequences=1
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ - ê¸¸ì´ ì œí•œ ì œê±°
            if "ë‹µë³€:" in generated_text:
                answer = generated_text.split("ë‹µë³€:")[-1].strip()
                
                # ìµœì†Œ ê¸¸ì´ë§Œ í™•ì¸ (ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ)
                if len(answer) > 5 and not self.is_repetitive(answer):
                    return answer
            
            return None
            
        except Exception as e:
            print(f"âŒ ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def is_repetitive(self, text):
        """ë°˜ë³µì ì¸ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ - ë” ê´€ëŒ€í•˜ê²Œ"""
        words = text.split()
        if len(words) < 6:  # ë„ˆë¬´ ì§§ìœ¼ë©´ ë°˜ë³µ ì²´í¬ ì•ˆí•¨
            return False
        
        # ê°™ì€ ë‹¨ì–´ê°€ ì—°ì†ìœ¼ë¡œ 4ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´ ë°˜ë³µì 
        for i in range(len(words) - 3):
            if words[i] == words[i+1] == words[i+2] == words[i+3]:
                return True
        
        return False
    
    def find_recipe_by_name(self, question):
        """ìš”ë¦¬ ì´ë¦„ìœ¼ë¡œ ë ˆì‹œí”¼ ì°¾ê¸°"""
        for recipe in self.recipes:
            if recipe['name'] in question:
                return recipe
        return None
    
    def find_recipes_by_ingredient(self, question):
        """ì¬ë£Œë¡œ ë ˆì‹œí”¼ ì°¾ê¸°"""
        matching_recipes = []
        for recipe in self.recipes:
            for ingredient in recipe['ingredients']:
                if len(ingredient) >= 2 and ingredient in question:
                    matching_recipes.append(recipe)
                    break
        return matching_recipes[:5]  # ìµœëŒ€ 5ê°œ
    
    def format_complete_recipe(self, recipe):
        """ì™„ì „í•œ ë ˆì‹œí”¼ ì •ë³´ í¬ë§·íŒ…"""
        result = f"ğŸ³ **{recipe['name']}**\n\n"
        
        # ì¹´í…Œê³ ë¦¬ì™€ ì¡°ë¦¬ë²• ì •ë³´
        if recipe.get('category'):
            result += f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {recipe['category']}\n"
        if recipe.get('cooking_method'):
            result += f"ğŸ”¥ ì¡°ë¦¬ë²•: {recipe['cooking_method']}\n"
        
        # ëª¨ë“  ì¬ë£Œ í‘œì‹œ
        if recipe['ingredients']:
            result += f"\nğŸ“‹ **ì¬ë£Œ ({len(recipe['ingredients'])}ê°œ):**\n"
            for i, ingredient in enumerate(recipe['ingredients'], 1):
                result += f"{i}. {ingredient}\n"
        
        # ëª¨ë“  ì¡°ë¦¬ ê³¼ì • í‘œì‹œ
        if recipe['steps']:
            result += f"\nğŸ‘¨â€ğŸ³ **ì¡°ë¦¬ ê³¼ì • ({len(recipe['steps'])}ë‹¨ê³„):**\n"
            for i, step in enumerate(recipe['steps'], 1):
                result += f"{i}. {step}\n"
        
        return result
    
    def format_ingredients_only(self, recipe):
        """ì¬ë£Œë§Œ ì™„ì „íˆ í‘œì‹œ"""
        if not recipe['ingredients']:
            return f"{recipe['name']}ì˜ ì¬ë£Œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        result = f"ğŸ¥¬ **{recipe['name']} ì¬ë£Œ ({len(recipe['ingredients'])}ê°œ):**\n\n"
        for i, ingredient in enumerate(recipe['ingredients'], 1):
            result += f"{i}. {ingredient}\n"
        
        return result
    
    def format_cooking_steps_only(self, recipe):
        """ì¡°ë¦¬ ê³¼ì •ë§Œ ì™„ì „íˆ í‘œì‹œ"""
        if not recipe['steps']:
            return f"{recipe['name']}ì˜ ì¡°ë¦¬ ë°©ë²• ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        result = f"ğŸ‘¨â€ğŸ³ **{recipe['name']} ë§Œë“œëŠ” ë²• ({len(recipe['steps'])}ë‹¨ê³„):**\n\n"
        for i, step in enumerate(recipe['steps'], 1):
            result += f"{i}. {step}\n"
        
        return result
    
    def fallback_answer(self, question):
        """ëª¨ë¸ ì‹¤íŒ¨ì‹œ ì™„ì „í•œ í´ë°± ë‹µë³€ - ì ˆëŒ€ ìë¥´ì§€ ì•ŠìŒ"""
        if not self.recipes:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 1. ë ˆì‹œí”¼ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰
        recipe = self.find_recipe_by_name(question)
        if recipe:
            # ì¡°ë¦¬ë²• ì§ˆë¬¸
            if any(word in question for word in ['ë§Œë“¤', 'ì¡°ë¦¬', 'ë°©ë²•', 'ì–´ë–»ê²Œ', 'ê³¼ì •', 'ìˆœì„œ']):
                return self.format_cooking_steps_only(recipe)
            
            # ì¬ë£Œ ì§ˆë¬¸
            elif any(word in question for word in ['ì¬ë£Œ', 'ë“¤ì–´ê°€', 'í•„ìš”', 'ë„£ì–´']):
                return self.format_ingredients_only(recipe)
            
            # ì „ì²´ ë ˆì‹œí”¼ ì§ˆë¬¸
            else:
                return self.format_complete_recipe(recipe)
        
        # 2. ì¬ë£Œë¡œ ê²€ìƒ‰
        matching_recipes = self.find_recipes_by_ingredient(question)
        if matching_recipes:
            if len(matching_recipes) == 1:
                recipe = matching_recipes[0]
                return self.format_complete_recipe(recipe)
            else:
                result = f"ğŸ” í•´ë‹¹ ì¬ë£Œë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ë“¤:\n\n"
                for i, recipe in enumerate(matching_recipes, 1):
                    result += f"{i}. **{recipe['name']}**\n"
                    # ì£¼ìš” ì¬ë£Œ ëª‡ ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
                    if recipe['ingredients']:
                        preview_ingredients = recipe['ingredients'][:3]
                        result += f"   ì¬ë£Œ: {', '.join(preview_ingredients)}"
                        if len(recipe['ingredients']) > 3:
                            result += f" ì™¸ {len(recipe['ingredients'])-3}ê°œ"
                        result += "\n"
                    result += "\n"
                
                result += "ğŸ’¡ êµ¬ì²´ì ì¸ ìš”ë¦¬ ì´ë¦„ì„ ë§ì”€í•˜ì‹œë©´ ìì„¸í•œ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë“œë ¤ìš”!"
                return result
        
        # 3. ì¼ë°˜ì ì¸ ìš”ë¦¬ ì¶”ì²œ
        if any(word in question for word in ['ì¶”ì²œ', 'ë­', 'ë¬´ì—‡', 'ì–´ë–¤']):
            popular_recipes = self.recipes[:10]  # ì²˜ìŒ 10ê°œ ë ˆì‹œí”¼
            result = "ğŸ³ **ì¸ê¸° ìš”ë¦¬ ì¶”ì²œ:**\n\n"
            for i, recipe in enumerate(popular_recipes, 1):
                result += f"{i}. {recipe['name']}"
                if recipe.get('category'):
                    result += f" ({recipe['category']})"
                result += "\n"
            
            result += "\nğŸ’¡ ì›í•˜ì‹œëŠ” ìš”ë¦¬ ì´ë¦„ì„ ë§ì”€í•˜ì‹œë©´ ìì„¸í•œ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë“œë ¤ìš”!"
            return result
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ ì´ë ‡ê²Œ ë¬¼ì–´ë³´ì„¸ìš”:\nâ€¢ 'ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?'\nâ€¢ 'ë¶ˆê³ ê¸° ì¬ë£Œê°€ ë­ì•¼?'\nâ€¢ 'ê°ìë¡œ ë­ ë§Œë“¤ ìˆ˜ ìˆì–´?'"
    
    def chat(self, question):
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€ - ì™„ì „í•œ ë‹µë³€ ì œê³µ"""
        if not question.strip():
            return "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë ˆì‹œí”¼ë‚˜ ìš”ë¦¬ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ³"
        
        # ì¸ì‚¬ë§ ì²˜ë¦¬
        if any(word in question for word in ['ì•ˆë…•', 'í—¬ë¡œ', 'ì²˜ìŒ', 'ë°˜ê°€ì›Œ']):
            model_status = "í›ˆë ¨ëœ ëª¨ë¸" if self.is_trained_model else "ê¸°ë³¸ ëª¨ë¸"
            return f"ì•ˆë…•í•˜ì„¸ìš”! KoGPT2 ë ˆì‹œí”¼ ì±—ë´‡ì…ë‹ˆë‹¤ ({model_status}). ìš”ë¦¬ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ³\n\nğŸ’¡ ì˜ˆì‹œ:\nâ€¢ 'ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?'\nâ€¢ 'ë¶ˆê³ ê¸° ì¬ë£Œê°€ ë­ì•¼?'\nâ€¢ 'ê°ìë¡œ ë­ ë§Œë“¤ ìˆ˜ ìˆì–´?'"
        
        # ë„ì›€ë§ ì²˜ë¦¬
        if any(word in question for word in ['ë„ì›€', 'ì‚¬ìš©ë²•', 'ëª…ë ¹ì–´']):
            return "ğŸ“– **ì‚¬ìš© ë°©ë²•:**\n\nğŸ³ **ì¡°ë¦¬ë²• ì§ˆë¬¸:**\nâ€¢ 'ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?'\nâ€¢ 'ë¶ˆê³ ê¸° ë§Œë“œëŠ” ë°©ë²•'\nâ€¢ 'ëœì¥ì°Œê°œ ì¡°ë¦¬ë²•'\n\nğŸ¥¬ **ì¬ë£Œ ì§ˆë¬¸:**\nâ€¢ 'ê¹€ì¹˜ì°Œê°œ ì¬ë£Œê°€ ë­ì•¼?'\nâ€¢ 'ë¶ˆê³ ê¸°ì— ë­ê°€ ë“¤ì–´ê°€?'\n\nğŸ” **ìš”ë¦¬ ê²€ìƒ‰:**\nâ€¢ 'ê°ìë¡œ ë­ ë§Œë“¤ ìˆ˜ ìˆì–´?'\nâ€¢ 'ë‹­ê³ ê¸° ìš”ë¦¬ ì¶”ì²œ'\n\nëª¨ë“  ì¡°ë¦¬ ê³¼ì •ê³¼ ì¬ë£Œë¥¼ ì™„ì „íˆ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤!"
        
        # KoGPT2ë¡œ ë‹µë³€ ìƒì„± ì‹œë„ (í›ˆë ¨ëœ ëª¨ë¸ì¸ ê²½ìš°)
        if self.is_trained_model:
            answer = self.generate_answer(question)
            
            if answer and len(answer.strip()) > 3:
                return answer
            else:
                print("âš ï¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ - ì™„ì „í•œ í´ë°± ë‹µë³€ ì‚¬ìš©")
        else:
            print("â„¹ï¸ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© - ì™„ì „í•œ í´ë°± ë‹µë³€ ì‚¬ìš©")
        
        # ì™„ì „í•œ í´ë°± ë‹µë³€
        return self.fallback_answer(question)

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸ¤– KoGPT2 ë ˆì‹œí”¼ ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print("=" * 60)
    
    chatbot = KoGPT2RecipeChatbot()
    
    test_questions = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?",
        "ê¹€ì¹˜ì°Œê°œ ì¬ë£Œê°€ ë­ì•¼?",
        "ê°ìë¡œ ë­ ë§Œë“¤ ìˆ˜ ìˆì–´?",
        "ìš”ë¦¬ ì¶”ì²œí•´ì¤˜",
        "ë„ì›€ë§"
    ]
    
    for question in test_questions:
        print(f"\nğŸ’¬ Q: {question}")
        answer = chatbot.chat(question)
        print(f"ğŸ¤– A: {answer}")
        print("-" * 60)