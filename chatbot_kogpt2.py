"""
KoGPT2 ê¸°ë°˜ ë ˆì‹œí”¼ ì±—ë´‡
"""
import json
import torch
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from config_kogpt2 import *

class KoGPT2RecipeChatbot:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.load_model()
        self.load_recipe_data()
    
    def load_model(self):
        """í›ˆë ¨ëœ KoGPT2 ëª¨ë¸ ë¡œë“œ"""
        try:
            # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
            self.model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)
            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_PATH)
            
            self.model.to(self.device)
            self.model.eval()
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… í›ˆë ¨ëœ KoGPT2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ KoGPT2 ëª¨ë¸ ì‚¬ìš©")
            
            # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            self.model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
            self.tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)
            
            self.model.to(self.device)
            self.model.eval()
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def load_recipe_data(self):
        """ë ˆì‹œí”¼ ë°ì´í„° ë¡œë“œ (í´ë°±ìš©)"""
        try:
            with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:
                self.recipes = json.load(f)
            print(f"ë ˆì‹œí”¼ ë°ì´í„° ë¡œë“œ: {len(self.recipes)}ê°œ")
        except:
            self.recipes = []
            print("ë ˆì‹œí”¼ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
    
    def generate_answer(self, question):
        """KoGPT2ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = PROMPT_FORMAT.format(question=question)
            
            # í† í¬ë‚˜ì´ì§•
            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                output = self.model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + GENERATION_MAX_LENGTH,
                    temperature=GENERATION_CONFIG['temperature'],
                    top_p=GENERATION_CONFIG['top_p'],
                    top_k=GENERATION_CONFIG['top_k'],
                    repetition_penalty=GENERATION_CONFIG['repetition_penalty'],
                    do_sample=GENERATION_CONFIG['do_sample'],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "ë‹µë³€:" in generated_text:
                answer = generated_text.split("ë‹µë³€:")[-1].strip()
                
                # ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°
                if len(answer) > 5:
                    return answer
            
            return None
            
        except Exception as e:
            print(f"ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def fallback_answer(self, question):
        """ëª¨ë¸ ì‹¤íŒ¨ì‹œ ê°„ë‹¨í•œ í´ë°± ë‹µë³€"""
        if not self.recipes:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ë ˆì‹œí”¼ ì´ë¦„ ê²€ìƒ‰
        for recipe in self.recipes:
            if recipe['name'] in question:
                if any(word in question for word in ['ë§Œë“¤', 'ì¡°ë¦¬', 'ë°©ë²•']):
                    return " ".join(recipe['steps']) if recipe['steps'] else "ì¡°ë¦¬ ë°©ë²• ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
                elif any(word in question for word in ['ì¬ë£Œ', 'ë“¤ì–´ê°€']):
                    return ", ".join(recipe['ingredients']) if recipe['ingredients'] else "ì¬ë£Œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë ˆì‹œí”¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def chat(self, question):
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€"""
        if not question.strip():
            return "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë ˆì‹œí”¼ë‚˜ ìš”ë¦¬ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ³"
        
        # ì¸ì‚¬ë§ ì²˜ë¦¬
        if any(word in question for word in ['ì•ˆë…•', 'í—¬ë¡œ', 'ì²˜ìŒ']):
            return "ì•ˆë…•í•˜ì„¸ìš”! ë ˆì‹œí”¼ ì±—ë´‡ì…ë‹ˆë‹¤. ìš”ë¦¬ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ³"
        
        # KoGPT2ë¡œ ë‹µë³€ ìƒì„± ì‹œë„
        answer = self.generate_answer(question)
        
        if answer and len(answer) > 5:
            return answer
        else:
            # í´ë°± ë‹µë³€
            print("ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ - í´ë°± ë‹µë³€ ì‚¬ìš©")
            return self.fallback_answer(question)

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    chatbot = KoGPT2RecipeChatbot()
    
    test_questions = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?",
        "ë¶ˆê³ ê¸°ì— ë­ê°€ ë“¤ì–´ê°€?",
        "ê°ìë¡œ ë­ ë§Œë“¤ ìˆ˜ ìˆì–´?",
        "ë¶€ëŒ€ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?",
        "ë¶ˆê³ ê¸° ì–´ë–»ê²Œ ë§Œë“¤ì–´? ìì„¸í•˜ê²Œ"
    ]
    
    print("ğŸ¤– KoGPT2 ì±—ë´‡ í…ŒìŠ¤íŠ¸!")
    print("=" * 50)
    
    for question in test_questions:
        answer = chatbot.chat(question)
        print(f"Q: {question}")
        print(f"A: {answer}\n")