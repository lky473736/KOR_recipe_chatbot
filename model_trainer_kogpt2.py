"""
KoGPT2 ëª¨ë¸ í›ˆë ¨ (íŒ¨ë”© í† í° ì˜¤ë¥˜ ì™„ì „ í•´ê²°)
"""
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from config_kogpt2 import *

class KoGPT2Dataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # í† í¬ë‚˜ì´ì €ê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        print(f"Dataset - íŒ¨ë”© í† í°: {self.tokenizer.pad_token}")
        print(f"Dataset - íŒ¨ë”© í† í° ID: {self.tokenizer.pad_token_id}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        
        # í† í¬ë‚˜ì´ì§• (ë” ì•ˆì „í•œ ë°©ë²•)
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        
        # GPT2ëŠ” input_idsì™€ labelsê°€ ë™ì¼
        labels = input_ids.clone()
        
        # íŒ¨ë”© í† í°ì— ëŒ€í•´ì„œëŠ” loss ê³„ì‚° ì•ˆí•¨
        labels[attention_mask == 0] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

class KoGPT2Trainer:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)
        
        # íŒ¨ë”© í† í° ê°•ì œ ì„¤ì • (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        print("í† í¬ë‚˜ì´ì € íŒ¨ë”© í† í° ì„¤ì • ì¤‘...")
        
        # ë°©ë²• 1: EOS í† í°ì„ íŒ¨ë”©ìœ¼ë¡œ ì‚¬ìš©
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # ë°©ë²• 2: ìƒˆë¡œìš´ íŒ¨ë”© í† í° ì¶”ê°€ (ìœ„ ë°©ë²•ì´ ì•ˆë˜ë©´)
        if self.tokenizer.pad_token is None:
            special_tokens_dict = {'pad_token': '<pad>'}
            self.tokenizer.add_special_tokens(special_tokens_dict)
            print("ìƒˆë¡œìš´ íŒ¨ë”© í† í° <pad> ì¶”ê°€")
        
        # ìµœì¢… í™•ì¸
        print(f"âœ… íŒ¨ë”© í† í° ì„¤ì • ì™„ë£Œ:")
        print(f"  - pad_token: {self.tokenizer.pad_token}")
        print(f"  - pad_token_id: {self.tokenizer.pad_token_id}")
        print(f"  - eos_token: {self.tokenizer.eos_token}")
        print(f"  - eos_token_id: {self.tokenizer.eos_token_id}")
        print(f"  - vocab_size: {len(self.tokenizer)}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
        
        # í† í° ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ì„ë² ë”© í¬ê¸° ì¡°ì •
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        self.model.to(self.device)
        
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}")
        
        # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
        self.test_tokenizer()
    
    def test_tokenizer(self):
        """í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
        test_text = "ì§ˆë¬¸: ê¹€ì¹˜ì°Œê°œ ì–´ë–»ê²Œ ë§Œë“¤ì–´?\në‹µë³€: ê¹€ì¹˜ì™€ ë¼ì§€ê³ ê¸°ë¥¼ ë³¶ìœ¼ì„¸ìš”.<|endoftext|>"
        
        try:
            encoding = self.tokenizer(
                test_text,
                truncation=True,
                padding='max_length',
                max_length=50,
                return_tensors='pt'
            )
            print("âœ… í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print(f"  - input_ids shape: {encoding['input_ids'].shape}")
            print(f"  - attention_mask shape: {encoding['attention_mask'].shape}")
            
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise e
    
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        with open(PROCESSED_DATA_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ì „ì²´ ë°ì´í„°: {len(data)}ê°œ")
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ê¸¸ì´ ì œí•œ
        valid_data = []
        for item in data:
            text = item.get('text', '')
            if text and len(text.strip()) > 10:
                # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸°
                if len(text) > 1000:
                    text = text[:1000] + END_TOKEN
                    item['text'] = text
                valid_data.append(item)
        
        print(f"ìœ íš¨í•œ ë°ì´í„°: {len(valid_data)}ê°œ")
        
        if len(valid_data) < 10:
            raise ValueError("ìœ íš¨í•œ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤!")
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        train_data, val_data = train_test_split(valid_data, test_size=0.1, random_state=42)
        
        # ë°ì´í„°ì…‹ ìƒì„± (ì´ë¯¸ ì„¤ì •ëœ í† í¬ë‚˜ì´ì € ì „ë‹¬)
        train_dataset = KoGPT2Dataset(train_data, self.tokenizer, MAX_LENGTH)
        val_dataset = KoGPT2Dataset(val_data, self.tokenizer, MAX_LENGTH)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader = DataLoader(
            train_dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=True,
            num_workers=0,
            pin_memory=False  # ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
        
        print(f"í›ˆë ¨: {len(train_data)}ê°œ, ê²€ì¦: {len(val_data)}ê°œ")
        print(f"ë°°ì¹˜ ìˆ˜: í›ˆë ¨ {len(train_loader)}, ê²€ì¦ {len(val_loader)}")
        
        return train_loader, val_loader
    
    def train(self):
        """ëª¨ë¸ í›ˆë ¨"""
        print("KoGPT2 ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        train_loader, val_loader = self.load_data()
        
        # ì˜µí‹°ë§ˆì´ì €
        optimizer = AdamW(self.model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
        
        best_loss = float('inf')
        
        for epoch in range(NUM_EPOCHS):
            print(f"\nğŸ“– Epoch {epoch+1}/{NUM_EPOCHS} ì‹œì‘")
            
            # í›ˆë ¨
            self.model.train()
            total_loss = 0
            num_batches = 0
            
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
            
            for batch_idx, batch in enumerate(progress_bar):
                try:
                    optimizer.zero_grad()
                    
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    # ë””ë²„ê·¸: ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ í™•ì¸
                    if batch_idx == 0 and epoch == 0:
                        print(f"\nğŸ” ì²« ë²ˆì§¸ ë°°ì¹˜ ì •ë³´:")
                        print(f"  - input_ids shape: {input_ids.shape}")
                        print(f"  - attention_mask shape: {attention_mask.shape}")
                        print(f"  - labels shape: {labels.shape}")
                    
                    # ëª¨ë¸ ì¶œë ¥
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    loss = outputs.loss
                    
                    # NaN ì²´í¬
                    if torch.isnan(loss):
                        print(f"âš ï¸ NaN loss ë°œìƒ (ë°°ì¹˜ {batch_idx})")
                        continue
                    
                    loss.backward()
                    
                    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    
                    optimizer.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                    
                    progress_bar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{total_loss/num_batches:.4f}'
                    })
                    
                except Exception as e:
                    print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            if num_batches > 0:
                avg_train_loss = total_loss / num_batches
                print(f"\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
                print(f"  - í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
                
                # ê²€ì¦
                val_loss = self.validate(val_loader)
                print(f"  - ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
                
                # ëª¨ë¸ ì €ì¥
                if val_loss < best_loss:
                    best_loss = val_loss
                    self.save_model()
                    print("  - âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥!")
                else:
                    print(f"  - ì„±ëŠ¥ ê°œì„  ì—†ìŒ (ìµœê³ : {best_loss:.4f})")
            else:
                print("âš ï¸ ìœ íš¨í•œ ë°°ì¹˜ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤")
            
            print("-" * 60)
    
    def validate(self, val_loader):
        """ëª¨ë¸ ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    if not torch.isnan(outputs.loss):
                        total_loss += outputs.loss.item()
                        num_batches += 1
                        
                except Exception as e:
                    continue
        
        return total_loss / max(num_batches, 1)
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(MODEL_PATH, exist_ok=True)
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥
        self.model.save_pretrained(MODEL_PATH)
        self.tokenizer.save_pretrained(MODEL_PATH)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_PATH}")

if __name__ == "__main__":
    try:
        trainer = KoGPT2Trainer()
        trainer.train()
        print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()